Ziya 6.25:
1. Finished implementing Network class basic methods
2. Created PlayPDG class

Ziya&James 6.26
1. Finished implementing Agent.strategyUpdate
2. Finished implementing PlayPDG.initializeNetwork

Ziya&James 6.27
1. Finished implementing calculatePayoffs
2. Finished implementing generate2D4N
2. Finished implementing agentRemoval

Ziya&James 6.28
1. Install Pajek on lab machine
2. Testing and debug: input graph into a visualizer and see our result

James 6.29
1, Finished implementing calculatePayoffsAll();strategyUpdateAll(); agentRemoveAll();
2, Update comment for several functions
3, Commented out part of code that we are not using as of right now.


TODO:
1, Create network into a txt file, ->txt file format output can be used for software to visualize it
2, Run cases for test(different value of alpha, etc.) -> create visualization for the results
3, always keep track of percentage of agents eliminated
4, percentage of cooperators left in network (in cooperator cluster)  -Amount of clusters left?
5, pick colors for cooperator and defector
6, use small network first to test the code
7, Done the coding part by Tuesday and start testing and run test cases after testing
8, PDG program read info from the text file -> a more efficient approach


Webpages Shared by Prof:
https://nwb.cns.iu.edu/
http://vlado.fmf.uni-lj.si/pub/networks/pajek/
https://r-graph-gallery.com/
https://www.aamas2024-conference.auckland.ac.nz/

Ziya&Jame 6.30
1. Finished adding index instance variable in agent class and change everything else related to it
2. Changed adjlists into List<Agent> instead of List<Edge>
3. Fixed bug for ConcurrentModification

Ziya&James 7.1
1.Fixed bugs in agentRemove, calculatePayoffs, playPDG, agentRemoveAll
2. Changed other detailed structures in our code, added a round() method to round decimals into certain digits for calculatePayoff function. 
3. Added more comments to explain inside different functions.
4. Made some rough tests on whether our code provide appropriate outcomes - we believe our code provide good results.

Ziya&James 7.3
1. Installed Pajek and txt2pajek in lab's machine
2. Figured out input file format and color of vertices
3. Did testing of PlayPDG.java and debugged

Ziya&James 7.4
1. Implemented writeNetworkToFile
2. Did more testing of PlayPDG.java
3. Kept track of aliveAgentCount
4. Changed adjList<Agent> to adjList<Integer> and other related methods
5. Did huge amount of debug

Ziya&James 7.5
1. Finished implementing writing to textfile that record fraction of defector&cooperator, and fraction of dead agents in every trial
2. Added comment to methods
3. Ran experiments and record results
4. Tried to fix bugs of infinite loop but did not find it

Ziya&James 7.6
1. Debugged the infinite loop by debugging the trategyUpdate method, import java.util.Random class and use Random imiIndex= new Random();, imiIndex.nextInt(4) to generate
random number
2. Ran more experiments to make sure the results are correct, but the results are not correct
3. IM

TODO:
1. pick 10 - 15 experiments to reproduce the figure 12, 13
2. Visualization 3 -  5 experiments from the 1
3. Check when one defector is in the center and if the result is consistent with the paper
4. Check when alpha=0, if all agents are dead to check if code is correct
5. Debug
6. Make the PPT
7. the RLBook Chapter1, Chapter 2, Chapter3

Ziya&James 7.7
1. Testing and try to find bugs
2. Testing and try to fix bugs
3. Testing but failed to find nor fix bugs

Ziya&James 7.10
1. Debug......
2. Sent email to Prof
3. Sent email to author of cascading paper to ask some questions

Ziya&James 7.12
1. Finished debugging and successfully reproduced results consistent with the cascading paper

Ziya&James 7.12
1. IM
2. Finish visualization of comparison between our results and the papers
3. Reorganize code to prepare for RL implementation

TODO:
1. Visualizations
- Comparison between our results and the papers to show our results are correct using a table 
- Comparison between paper, Q Learning & Double Q Learning
- Comparison of total payoffs of all agents when they are RL agents and when they are all cooperators 

2. Experiments
- Reinforcement learning up to ten different experiments, and pick 5 
b = 1[0.2, 0.4, 0.7] - we use this one to debug 
b = 1.05 [0.2, 0.4, 0.7] 
b = 1.1 [0.2, 0.4, 0.7] 
- For when a single defector in the center for the paper, and when a single RL agent in the center (Q learning & Double Q Learning)
- Track D|C, C|C, D|D and see if it flattens; if flattens, then our code is probably correct 
- Track percentage of RL agents and normal cooperating agents 

Algorithm:
1. Agent class have another boolean activated: T if RL agent, F if cooperating agent
2. If RL <-> RL, then we don't update either's strategy; only if RL<-> normal agent, we update strategy with probability Wij 
3. Initialize network with one RL agent in the middle, where everyone else is cooperating agents


