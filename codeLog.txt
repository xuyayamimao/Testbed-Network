Ziya 6.25:
1. Finished implementing Network class basic methods
2. Created PlayPDG class

Ziya&James 6.26
1. Finished implementing Agent.strategyUpdate
2. Finished implementing PlayPDG.initializeNetwork

Ziya&James 6.27
1. Finished implementing calculatePayoffs
2. Finished implementing generate2D4N
2. Finished implementing agentRemoval

Ziya&James 6.28
1. Install Pajek on lab machine
2. Testing and debug: input graph into a visualizer and see our result

James 6.29
1, Finished implementing calculatePayoffsAll();strategyUpdateAll(); agentRemoveAll();
2, Update comment for several functions
3, Commented out part of code that we are not using as of right now.


TODO:
1, Create network into a txt file, ->txt file format output can be used for software to visualize it
2, Run cases for test(different value of alpha, etc.) -> create visualization for the results
3, always keep track of percentage of agents eliminated
4, percentage of cooperators left in network (in cooperator cluster)  -Amount of clusters left?
5, pick colors for cooperator and defector
6, use small network first to test the code
7, Done the coding part by Tuesday and start testing and run test cases after testing
8, PDG program read info from the text file -> a more efficient approach


Webpages Shared by Prof:
https://nwb.cns.iu.edu/
http://vlado.fmf.uni-lj.si/pub/networks/pajek/
https://r-graph-gallery.com/
https://www.aamas2024-conference.auckland.ac.nz/

Ziya&Jame 6.30
1. Finished adding index instance variable in agent class and change everything else related to it
2. Changed adjlists into List<Agent> instead of List<Edge>
3. Fixed bug for ConcurrentModification

Ziya&James 7.1
1.Fixed bugs in agentRemove, calculatePayoffs, playPDG, agentRemoveAll
2. Changed other detailed structures in our code, added a round() method to round decimals into certain digits for calculatePayoff function. 
3. Added more comments to explain inside different functions.
4. Made some rough tests on whether our code provide appropriate outcomes - we believe our code provide good results.

Ziya&James 7.3
1. Installed Pajek and txt2pajek in lab's machine
2. Figured out input file format and color of vertices
3. Did testing of PlayPDG.java and debugged

Ziya&James 7.4
1. Implemented writeNetworkToFile
2. Did more testing of PlayPDG.java
3. Kept track of aliveAgentCount
4. Changed adjList<Agent> to adjList<Integer> and other related methods
5. Did huge amount of debug

Ziya&James 7.5
1. Finished implementing writing to textfile that record fraction of defector&cooperator, and fraction of dead agents in every trial
2. Added comment to methods
3. Ran experiments and record results
4. Tried to fix bugs of infinite loop but did not find it

Ziya&James 7.6
1. Debugged the infinite loop by debugging the trategyUpdate method, import java.util.Random class and use Random imiIndex= new Random();, imiIndex.nextInt(4) to generate
random number
2. Ran more experiments to make sure the results are correct, but the results are not correct
3. IM

TODO:
1. pick 10 - 15 experiments to reproduce the figure 12, 13
2. Visualization 3 -  5 experiments from the 1
3. Check when one defector is in the center and if the result is consistent with the paper
4. Check when alpha=0, if all agents are dead to check if code is correct
5. Debug
6. Make the PPT
7. the RLBook Chapter1, Chapter 2, Chapter3

Ziya&James 7.7
1. Testing and try to find bugs
2. Testing and try to fix bugs
3. Testing but failed to find nor fix bugs

Ziya&James 7.10
1. Debug......
2. Sent email to Prof
3. Sent email to author of cascading paper to ask some questions

Ziya&James 7.12
1. Finished debugging and successfully reproduced results consistent with the cascading paper

Ziya&James 7.12
1. IM
2. Finish visualization of comparison between our results and the papers
3. Reorganize code to prepare for RL implementation

TODO:
1. Visualizations
- Comparison between our results and the papers to show our results are correct using a table 
- Comparison between paper, Q Learning & Double Q Learning
- Comparison of total payoffs of all agents when they are RL agents and when they are all cooperators 

2. Experiments
- Reinforcement learning up to ten different experiments, and pick 5 
b = 1[0.2, 0.4, 0.7] - we use this one to debug 
b = 1.05 [0.2, 0.4, 0.7] 
b = 1.1 [0.2, 0.4, 0.7] 
- For when a single defector in the center for the paper, and when a single RL agent in the center (Q learning & Double Q Learning)
- Track D|C, C|C, D|D and see if it flattens; if flattens, then our code is probably correct 
- Track percentage of RL agents and normal cooperating agents 

Algorithm:
1. Agent class have another boolean activated: T if RL agent, F if cooperating agent
2. If RL <-> RL, then we don't update either's strategy; only if RL<-> normal agent, we update strategy with probability Wij 
3. Initialize network with one RL agent in the middle, where everyone else is cooperating agents

TODO:
1.Testing
- experiment1: b = 0, alpha = 0; and see if all alive agents become Rl agents, and all Rl agents learn to cooperate (because incentive to defect = 0)
total payoff will decraese a bit from when all agents are cooperators, will fluctuate around this value a bit, if equilibrium is achieved, payoff shouldn't change anymore
- experiment2: b=0, alpha=0.2 and see if RL agents can learn to cooperate, depend on clock,maybe not enough time is given to agents for learning. If weird things
happened, check the clock, make it larger
unless the exploration of agents cause deaths to other agents, total payoff will also achieve constant when equilibrium is achieved
(above two experiments help us to check implementation)
- experiment 3: b = 1, alpha = 0.2
(experiment 3 are when we are good to go)
2. How is RL agents spreading, too slow or too quick
3. trainingPeriod clock = 100 (exploration allowed when iteratiion < 100) -> see if this number is too large or too small
4. implement ifSteady
5. what data we want to collect: alivePercentage, defectorPercentage, cooperatorPercentage, C|D pair, sum of payoff of every agents per trial (x, trial, y sum of payoff)
what would be if every agents play as cooperator, what would be the total payoff(maximum possible payoff), calculate this for every trial/round


Ziya&James 7.23
1. Finished implemented ifSteady, clock of each agent
2. Optimized strategyUpdateAll and deleted strategyUpdate
3. Ran tests of b = 0, alpha = 0 and b=0, alpha=0.2


Ziya&James 7.24 
Testing of Phase1:
1. experiment1: b = 0, alpha = 0: all 10000 alive agents are RL agents, 9968 of them are cooperators when the game ends in equilibrium 
2. experiment2: b=0, alpha=0.2:
   - when the first RL agent is initialized to be cooperator, it spreads the RL strategy to around 135 of other agents, but all RL agents eventually died when they explore to become defector, so the RL strategy stopped spreading -> the game ends in a loop
   - when the first RL agent is initialized to be defector, it immedialy dies after the first round-> RL strategy stopped spreading with all other 99 agents as non-RL agents-> the game enter infinite loop
3. experiment3: b = 0.9, alpha = 0.2:
   - when the first agent is initialized to be a cooperator, the game ends in equilibrium, with the data in the final round as:
	Round1237:
	alive agent num: 3188
	RL agent num: 3188
	expired clock num 3188
	cooperator count: 2024
     	
   - when the first agent is initialized to be a defector:
	Round1237:
	alive agent num: 3300
	RL agent num: 3300
	expired clock num 3300
	cooperator count: 2045

4. experiment4: b = 1, alpha = 0.2:
   - when the first agent is initialized to be a cooperator, the game ends in equilibrium, with the data in the final round as:
	alive agent num: 2697
	RL agent num: 2697
	expired clock num 2697
	cooperator count: 1619
   - when the first agent is initialized to be a defector:
	alive agent num: 2821
	RL agent num: 2821
	expired clock num 2821
	cooperator count: 1660

Ziya&James 7.26
Visualizations:
1. D|C, C|C, D|D of one experiment in one line plot using different color | total 8 plots
2. x: trials. y: Percentage of alive agents per trial of one experiment in one line plot | total 8 plots
3. x: trials. y: The percentage of alive RL agents per trial of one experiment in one line plot | total 8 plots
4. x: trials. y: sum of agents' payoffs per trial | total 8 plots 

Total 8 excel forms with one experiment results in one form 

TODO:
1. Pay attention to the scale of different visualizations, make sure they are consistent
2. Percentage of alive agent && payoffSum side by side in the paper
3. CC, DC, DD and Percentage of RL agents side by side (RL agents 
3. Alive agent as one graph
4. RL agent percentage as one graph
5. In the CC DC DD plot, have a vertical line to show when all agents have become RL agents
6. Calculate data of percentage of dumb agent by (AliveAgentPercentage - RLAgentPercentage)
7. Choose one interesting simulation to create a network using Pajek, simulation ended up with fewer number of agents e.g. b = 1, alpha with fewer number of agents 
8. Run experiments for 100 simulations -> and make visualizations for the average of them
9. Have a counter for the number of cascading failures in the simulations, not include data for cascading failure in visualization, but we will tell the audience the percentage of cascading failures 

Experiments, each with different parameter:
1. Combo Q-Learning
2. Combo Double Q-Learning
3. Everybody as Q-Learning RL agents
4. everybody as Double-Q Learning (if we have the time)

For 1,3, and 2,3, make sure the parameters (e.g. clock&spread rate) are consistent 

For testing two combos
0.9, 0.2
1, 0.2
1.1 0.2
For testing everybody as Q-Learning
